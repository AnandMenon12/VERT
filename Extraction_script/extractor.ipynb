{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def split(delimiters, string, maxsplit=0):    #simple method that takes a list of delimiters, the string to be split, and an optional max amount of splits and splits the string accordingly\n",
    "    regex_pattern = '|'.join(map(re.escape, delimiters))\n",
    "    return re.split(regex_pattern, string, maxsplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File opener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If, case, and always statement extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def ifCaseFinder(currentFile):\n",
    "    ifCaseFound = 0         # counter for if/case nesting\n",
    "    alwaysCheck = 0         # checking if synchronous\n",
    "    currentLine = 0         # counter for current line\n",
    "    elseFound = 0           # checking if next line is an else statement\n",
    "    multiLine = False\n",
    "    fileString = \"\"         # stores the current if/case block text\n",
    "    syncArgText = \"\"        # arguments of always statement\n",
    "    ifCaseArgText = \"\"      # arguments of if/case statement\n",
    "    variables = ([],[])     # output/input, variable (unchanged in this version)\n",
    "    fileStrings = ([], [], [], [])  # asynchronous/synchronous, sync args, if/case args, full block text\n",
    "\n",
    "    while currentLine < len(currentFile):\n",
    "        # Look for the start of an if/case block\n",
    "        while ifCaseFound == 0 and currentLine < len(currentFile):\n",
    "            currentLineText = currentFile[currentLine].split(\"//\", 1)[0]\n",
    "            \n",
    "            # Check for an always block indicator (@) and extract its arguments\n",
    "            if \"@\" in currentLineText and \"*\" not in currentLineText:\n",
    "                m = re.search(r'\\((.*?)\\)', currentLineText)\n",
    "                if m:\n",
    "                    syncArgText = m.group(1)\n",
    "                alwaysCheck = 1\n",
    "\n",
    "            # Check for leaving an always block\n",
    "            if \"\\tend\" in currentLineText or \"\\nend\" in currentLineText or \" end\" in currentLineText:\n",
    "                syncArgText = \"\"\n",
    "                alwaysCheck = 0\n",
    "\n",
    "            # Check for an if or case statement (various spacing variations)\n",
    "            if ((\" if \" in currentLineText or \" if(\" in currentLineText or \"\\tif \" in currentLineText or \"\\nif \" in currentLineText) or\n",
    "                (\" case\" in currentLineText or \"\\tcase\" in currentLineText or \"\\ncase\" in currentLineText)):\n",
    "                m = re.search(r'\\((.*?)\\)', currentLineText)\n",
    "                if m:\n",
    "                    ifCaseArgText = m.group(1)\n",
    "                fileString += currentLineText  # start accumulating the block text\n",
    "                ifCaseFound += 1\n",
    "            currentLine += 1\n",
    "\n",
    "        if currentLine >= len(currentFile):\n",
    "            break\n",
    "\n",
    "        # Process the if/case block until its end\n",
    "        while (ifCaseFound != 0 or elseFound == 1) and currentLine < len(currentFile):\n",
    "            elseFound = 0\n",
    "            currentLineText = currentFile[currentLine].split(\"//\", 1)[0]\n",
    "\n",
    "            # Handle multi-line comments\n",
    "            if multiLine:\n",
    "                if \"*/\" in currentLineText:\n",
    "                    multiLine = False\n",
    "                currentLine += 1\n",
    "                continue\n",
    "\n",
    "            if \"/*\" in currentLineText:\n",
    "                multiLine = True\n",
    "                currentLineText = currentLineText.split(\"/*\", 1)[0]\n",
    "\n",
    "            # Ensure line ends with a newline character\n",
    "            if \"\\n\" not in currentLineText:\n",
    "                currentLineText += \"\\n\"\n",
    "            fileString += currentLineText\n",
    "\n",
    "            # Increase/decrease nesting based on keywords\n",
    "            if \"begin\" in currentLineText or \"case\" in currentLineText:\n",
    "                ifCaseFound += 1\n",
    "            if (\"end\" in currentLineText or \"endcase\" in currentLineText):\n",
    "                ifCaseFound -= 1\n",
    "\n",
    "            currentLine += 1\n",
    "\n",
    "            # If the block ended and the next line is an \"else\", include it in the same block\n",
    "            if currentLine < len(currentFile) and ifCaseFound == 0:\n",
    "                nextLine = currentFile[currentLine].split(\"//\", 1)[0]\n",
    "                if \"else \" in nextLine:\n",
    "                    elseFound = 1\n",
    "\n",
    "        # Store the collected data\n",
    "        fileStrings[0].append(alwaysCheck)\n",
    "        fileStrings[1].append(syncArgText)\n",
    "        fileStrings[2].append(ifCaseArgText)\n",
    "        fileStrings[3].append(fileString)\n",
    "        fileString = \"\"  # reset for next block\n",
    "\n",
    "    variables = np.array(variables)\n",
    "    fileStrings = np.array(fileStrings)\n",
    "    return [variables, fileStrings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file_name):\n",
    "    file_object = open(file_name, 'r')    #open the requested file on \"read\"\n",
    "    currentFile = file_object.readlines()             #turn file into an array of strings (one per line)\n",
    "    fileStrings = ifCaseFinder(currentFile)     #call ifCaseFinder and store the array returned in fileStrings\n",
    "    file_object.close()                               #close the file\n",
    "    return fileStrings                          #return the array fileStrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "gpu_id = \"cuda:0\"\n",
    "base_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "\n",
    "\n",
    "device = torch.device(gpu_id)\n",
    "torch.cuda.set_device(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./Models/DeepSeek-6-7b-instruct-Verilog-sync\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True).to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map= gpu_id\n",
    ")\n",
    "\n",
    "adapter_name = model.load_adapter(\"./Models/DeepSeek-6-7b-instruct-Verilog-sync\")\n",
    "model.active_adapters = adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = [stop.to(\"cuda:0\") for stop in stops]\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_token = input_ids[0][-1] # the generated token\n",
    "        for stop in self.stops: # stop words\n",
    "            if tokenizer.decode(stop[-1]) == tokenizer.decode(last_token): # if the generated token is in the stop words\n",
    "                # print (\"HIT\")\n",
    "                # print(tokenizer.decode(stop[-1]))\n",
    "                return True # stop the generation\n",
    "        return False\n",
    "\n",
    "\n",
    "stop_words = [\"EOT\"]\n",
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in stop_words]\n",
    "print(stop_words_ids)\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files to be scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "namlist = [\"\"]\n",
    "\n",
    "for name in namlist:\n",
    "    filedirectory = \"./RTL_deepseek/\"+ name         #directory of the files\n",
    "    #filedirectory = \"./TEST\"         #directory of the files\n",
    "    filesToOpen = []                    #array to hold all the files\n",
    "    # in a loop read all the files and store the data in an array\n",
    "\n",
    "    for file_name in os.listdir(filedirectory):    #loop through all files in the directory\n",
    "        if file_name.endswith(\".sv\") or file_name.endswith(\".v\"):                #only open .sv files\n",
    "            filesToOpen.append(filedirectory + \"/\" + file_name)    #add the files to the array\n",
    "\n",
    "    print(\"Files being asserted\", filesToOpen)\n",
    "    for file_name in filesToOpen:\n",
    "        ifElseCaseArray = []                #array to hold each file's data separately\n",
    "        print(file_name) # print the file name\n",
    "        ifElseCaseArray = readFile(file_name)    #call readFile and store the data in ifElseCaseArray\n",
    "        clk_condition = 0\n",
    "        with open(file_name + \".txt\", \"w\") as f:\n",
    "            for code in ifElseCaseArray[1][3]:\n",
    "                if(len(ifElseCaseArray[1][1][clk_condition]) != 0) and  len(code) < 500: # Sunchronous\n",
    "                    PromptString = \"\"\n",
    "                    PromptString += \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer. \\n### Instruction:\"\n",
    "                    PromptString += \"Generate a list of synchronous systemverilog assertion executing at \"\n",
    "                    PromptString += str(' '.join(ifElseCaseArray[1][1][clk_condition].split()[:2]))   \n",
    "                    PromptString += \" from the following code \\n\"\n",
    "                    PromptString += str(code)\n",
    "                    PromptString += \"\\n ### Response: \\n\\n\"\n",
    "                    print(PromptString)\n",
    "                    input_text = PromptString\n",
    "                    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                    outputs = model.generate(**inputs, max_length=5000,stopping_criteria=stopping_criteria, \n",
    "                                            eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "                    output = tokenizer.decode(outputs[0],  # remove the token the LLM stops at\n",
    "                                        skip_special_tokens=True)\n",
    "                    print(output[:-1])\n",
    "                    f.write(output[:-1])\n",
    "                elif len(ifElseCaseArray[1][1][clk_condition]) == 0 and  len(code) < 500: # Asynchronous\n",
    "                    PromptString = \"\"\n",
    "                    PromptString += \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer. \\n### Instruction:\"\n",
    "                    PromptString += \"Generate a list of asynchronous systemverilog assertion\"\n",
    "                    PromptString += \" from the following code \\n\"\n",
    "                    PromptString += str(code)\n",
    "                    PromptString += \"\\n ### Response: \\n\\n\"\n",
    "                    print(PromptString)\n",
    "                    input_text = PromptString\n",
    "                    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "                    outputs = model.generate(**inputs, max_length=5000,stopping_criteria=stopping_criteria, \n",
    "                                            eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "                    #output = tokenizer.decode(outputs[0][7:-2],  # remove the token the LLM stops at\n",
    "                    #                    skip_special_tokens=True)[len(input_text):]\n",
    "                    output = tokenizer.decode(outputs[0],  # remove the token the LLM stops at\n",
    "                                        skip_special_tokens=True)\n",
    "                    print(output[:-1])\n",
    "                    f.write(output[:-1])\n",
    "                clk_condition += 1\n",
    "        \n",
    "    # make a directory to store the files\n",
    "    foldername = filedirectory + \"/\" + name + \"_assertions\"\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "\n",
    "    os.system(\"mv \" + filedirectory + \"/*.txt \" + foldername) # move all the files to the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the extracted statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
